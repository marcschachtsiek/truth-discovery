{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def results_to_dataframe(exp_name, in_parts, postfix_list=None, folder=None, return_optional=False):\n",
    "    \n",
    "    rows = []\n",
    "    index = 0\n",
    "    \n",
    "    n_parts = 5 if in_parts else 1\n",
    "    if postfix_list is not None and len(postfix_list) != n_parts:\n",
    "        raise ValueError(\"Length of postfix list must match number of parts\")\n",
    "        \n",
    "    for i in range(n_parts):\n",
    "        if postfix_list is not None:\n",
    "            pf_list = ['', postfix_list[i]]\n",
    "        else:\n",
    "            pf_list = ['']\n",
    "\n",
    "        for postfix in pf_list:\n",
    "        \n",
    "            name = f\"{exp_name}-part{i}\" if in_parts else exp_name\n",
    "            if folder is None:\n",
    "                path = os.path.join(\"results\", \"raw\", name + postfix + \".json\")\n",
    "            else:\n",
    "                path = os.path.join(\"results\", \"raw\", folder, name + postfix + \".json\")\n",
    "\n",
    "            with open(path, \"r\") as f:\n",
    "                results = json.load(f)\n",
    "                \n",
    "            dictionary = {\n",
    "                'n_sources': results['n_sources'],\n",
    "                'n_dataitems': results['n_dataitems'],\n",
    "                'n_distinct': results['n_distinct'],\n",
    "            }\n",
    "\n",
    "            algorithms = {}\n",
    "            for algo in results['algorithms_info']:\n",
    "                algorithms[algo['name']] = algo\n",
    "\n",
    "            for experiment in results['experiments']:\n",
    "                for dist in ['coverage_dist', 'truth_dist', 'distinct_dist', 'spread_dist']:\n",
    "                    prefix = \"\"\n",
    "                    if 'flipped' in experiment[dist] and experiment[dist]['flipped']:\n",
    "                        prefix += \"F_\"\n",
    "                    if experiment[dist]['name'] == 'TruncExponential' and experiment[dist]['lmbda'] == 15:\n",
    "                        prefix += \"S_\"\n",
    "                    if experiment[dist]['name'] == 'TruncPareto' and experiment[dist]['alpha'] == 25.3:\n",
    "                        prefix += \"S_\"\n",
    "                    dictionary[dist] = prefix + experiment[dist]['name']\n",
    "\n",
    "                dictionary['optimal_perc_score'] = experiment['optimal'][0]\n",
    "                dictionary['optimal_score'] = experiment['optimal'][1]\n",
    "                dictionary['n_claims'] = experiment['n_claims']\n",
    "                dictionary['iteration_index'] = experiment['iteration_index'] if 'iteration_index' in experiment else -1\n",
    "\n",
    "                for key, value in experiment['results'].items():\n",
    "                    dictionary[f'{key}_time'] = value['time']\n",
    "                    dictionary[f'{key}_score_perc'] = value['scores'][0]\n",
    "                    dictionary[f'{key}_score'] = value['scores'][1]\n",
    "                \n",
    "                rows.append(pd.DataFrame.from_dict({index: dictionary}, orient='index'))\n",
    "                index += 1\n",
    "        \n",
    "    if return_optional:\n",
    "        return pd.concat(rows), algorithms\n",
    "    \n",
    "    return pd.concat(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run1-1-5-5\n",
      "run1-1-5-20\n",
      "run1-2-2-5\n",
      "run1-2-2-20\n",
      "run1-5-1-5\n",
      "run1-5-1-20\n",
      "run2-1-5-5\n",
      "run2-1-5-20\n",
      "run2-2-2-5\n",
      "run2-2-2-20\n",
      "run2-5-1-5\n",
      "run2-5-1-20\n",
      "run3-1-5-5\n",
      "run3-1-5-20\n",
      "run3-2-2-5\n",
      "run3-2-2-20\n",
      "run3-5-1-5\n",
      "run3-5-1-20\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "\n",
    "runs = [1, 2, 3]\n",
    "combs = [(1, 5), (2, 2), (5, 1)]\n",
    "dist = [5, 20]\n",
    "parts_list = [True, True, True, False, False, False,\n",
    "              True, True, True, False, False, False,\n",
    "              True, True, False, False, False, False]\n",
    "\n",
    "for (run, (s, di), dist), parts in zip(product(runs, combs, dist), parts_list):\n",
    "    exp_name = f\"run{run}-{s}-{di}-{dist}\"\n",
    "    print(exp_name)\n",
    "    df = results_to_dataframe(exp_name, in_parts=parts, folder=exp_name)\n",
    "    df.to_csv(os.path.join(\"results\", \"raw\", f\"{exp_name}.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "truth1-1-5-5-exttruth\n",
      "(500, 14)\n",
      "truth1-1-5-20-exttruth\n",
      "(500, 14)\n",
      "truth1-2-2-5-exttruth\n",
      "(500, 14)\n",
      "truth1-2-2-20-exttruth\n",
      "(500, 14)\n",
      "truth1-5-1-5-exttruth\n",
      "(500, 14)\n",
      "truth1-5-1-20-exttruth\n",
      "(500, 14)\n",
      "truth2-1-5-5-exttruth\n",
      "(500, 14)\n",
      "truth2-1-5-20-exttruth\n",
      "(500, 14)\n",
      "truth2-2-2-5-exttruth\n",
      "(500, 14)\n",
      "truth2-2-2-20-exttruth\n",
      "(500, 14)\n",
      "truth2-5-1-5-exttruth\n",
      "(500, 14)\n",
      "truth2-5-1-20-exttruth\n",
      "(500, 14)\n",
      "truth3-1-5-5-exttruth\n",
      "(500, 14)\n",
      "truth3-1-5-20-exttruth\n",
      "(500, 14)\n",
      "truth3-2-2-5-exttruth\n",
      "(500, 14)\n",
      "truth3-2-2-20-exttruth\n",
      "(500, 14)\n",
      "truth3-5-1-5-exttruth\n",
      "(500, 14)\n",
      "truth3-5-1-20-exttruth\n",
      "(500, 14)\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "\n",
    "runs = [1, 2, 3]\n",
    "combs = [(1, 5), (2, 2), (5, 1)]\n",
    "dist = [5, 20]\n",
    "parts = False\n",
    "\n",
    "for run, (s, di), dist in product(runs, combs, dist):\n",
    "    exp_name = f\"truth{run}-{s}-{di}-{dist}-exttruth\"\n",
    "    print(exp_name)\n",
    "    df = results_to_dataframe(exp_name, in_parts=parts, folder=\"truth_extension\")\n",
    "    print(df.shape)\n",
    "    df.to_csv(os.path.join(\"results\", \"raw\", f\"{exp_name}.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "runs = [1, 2, 3]\n",
    "combinations = [(1, 5), (2, 2), (5, 1)]\n",
    "distinct = [5, 20]\n",
    "\n",
    "truth_extension = True\n",
    "\n",
    "df_dict = {}\n",
    "for r, c, d in product(runs, combinations, distinct):\n",
    "    if truth_extension:\n",
    "        df_dict[(r, c, d)] = pd.read_csv(os.path.join(\"results\", \"raw\", f\"truth{r}-{c[0]}-{c[1]}-{d}-exttruth.csv\"))\n",
    "    else:\n",
    "        df_dict[(r, c, d)] = pd.read_csv(os.path.join(\"results\", \"raw\", f\"run{r}-{c[0]}-{c[1]}-{d}.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if truth_extension:\n",
    "    algos = [\"TwoEstimates\"]\n",
    "else:\n",
    "    algos = [\"Majority\", \"TruthFinder\", \"TwoEstimates\", \"ThreeEstimates\"]\n",
    "options = [\"time\", \"score_perc\", \"score\"]\n",
    "extras = [\"optimal_perc_score\", \"optimal_score\", \"n_claims\"]\n",
    "\n",
    "df_final = {}\n",
    "for r in runs:\n",
    "    renamer = {}\n",
    "    for al, op in product(algos, options):\n",
    "        if al == \"optimal\" and op == \"time\":\n",
    "            continue\n",
    "        renamer[f\"{al}_{op}\"] = f\"{al}_{op}_{r}\"\n",
    "    \n",
    "    for ex in extras:\n",
    "        renamer[f\"{ex}\"] = f\"{ex}_{r}\"\n",
    "\n",
    "    for c, d in product(combinations, distinct):\n",
    "        df_dict[(r, c, d)] = df_dict[(r, c, d)].rename(columns=renamer, errors=\"raise\").drop(\"iteration_index\", axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys([((1, 5), 5), ((1, 5), 20), ((2, 2), 5), ((2, 2), 20), ((5, 1), 5), ((5, 1), 20)])\n"
     ]
    }
   ],
   "source": [
    "merge_on = ['n_sources', 'n_dataitems', 'n_distinct', 'coverage_dist', 'truth_dist', 'distinct_dist', 'spread_dist']\n",
    "\n",
    "df_final = {}\n",
    "for c, d in product(combinations, distinct):\n",
    "    df_final[(c, d)] = df_dict[(1, c, d)].merge(df_dict[(2, c, d)], on=merge_on, validate=\"one_to_one\").merge(df_dict[(3, c, d)], on=merge_on, validate=\"one_to_one\")\n",
    "    \n",
    "print(df_final.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c, d in product(combinations, distinct):\n",
    "    for al, op in product(algos, options):\n",
    "        df_final[(c, d)][f\"{al}_{op}\"] = (df_final[(c, d)][f\"{al}_{op}_{1}\"] + df_final[(c, d)][f\"{al}_{op}_{2}\"] + df_final[(c, d)][f\"{al}_{op}_{3}\"]) / 3.0\n",
    "    \n",
    "    for ex in extras:\n",
    "        df_final[(c, d)][f\"{ex}\"] = (df_final[(c, d)][f\"{ex}_{1}\"] + df_final[(c, d)][f\"{ex}_{2}\"] + df_final[(c, d)][f\"{ex}_{3}\"]) / 3.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 31)\n",
      "(500, 31)\n",
      "(500, 31)\n",
      "(500, 31)\n",
      "(500, 31)\n",
      "(500, 31)\n"
     ]
    }
   ],
   "source": [
    "for k in df_final:\n",
    "    print(df_final[k].shape)\n",
    "    if truth_extension:\n",
    "        df_final[k].to_csv(os.path.join(\"results\", f\"truth_final_{k[0][0] * 100}_{k[0][1] * 100}_{k[1]}.csv\"), index=False)\n",
    "    else:\n",
    "        df_final[k].to_csv(os.path.join(\"results\", f\"final_{k[0][0] * 100}_{k[0][1] * 100}_{k[1]}.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['n_sources', 'n_dataitems', 'n_distinct', 'coverage_dist', 'truth_dist',\n",
      "       'distinct_dist', 'spread_dist', 'optimal_perc_score_1',\n",
      "       'optimal_score_1', 'n_claims_1', 'Majority_time_1',\n",
      "       'Majority_score_perc_1', 'Majority_score_1', 'TruthFinder_time_1',\n",
      "       'TruthFinder_score_perc_1', 'TruthFinder_score_1',\n",
      "       'TwoEstimates_time_1', 'TwoEstimates_score_perc_1',\n",
      "       'TwoEstimates_score_1', 'ThreeEstimates_time_1',\n",
      "       'ThreeEstimates_score_perc_1', 'ThreeEstimates_score_1',\n",
      "       'optimal_perc_score_2', 'optimal_score_2', 'n_claims_2',\n",
      "       'Majority_time_2', 'Majority_score_perc_2', 'Majority_score_2',\n",
      "       'TruthFinder_time_2', 'TruthFinder_score_perc_2', 'TruthFinder_score_2',\n",
      "       'TwoEstimates_time_2', 'TwoEstimates_score_perc_2',\n",
      "       'TwoEstimates_score_2', 'ThreeEstimates_time_2',\n",
      "       'ThreeEstimates_score_perc_2', 'ThreeEstimates_score_2',\n",
      "       'optimal_perc_score_3', 'optimal_score_3', 'n_claims_3',\n",
      "       'Majority_time_3', 'Majority_score_perc_3', 'Majority_score_3',\n",
      "       'TruthFinder_time_3', 'TruthFinder_score_perc_3', 'TruthFinder_score_3',\n",
      "       'TwoEstimates_time_3', 'TwoEstimates_score_perc_3',\n",
      "       'TwoEstimates_score_3', 'ThreeEstimates_time_3',\n",
      "       'ThreeEstimates_score_perc_3', 'ThreeEstimates_score_3',\n",
      "       'Majority_time', 'Majority_score_perc', 'Majority_score',\n",
      "       'TruthFinder_time', 'TruthFinder_score_perc', 'TruthFinder_score',\n",
      "       'TwoEstimates_time', 'TwoEstimates_score_perc', 'TwoEstimates_score',\n",
      "       'ThreeEstimates_time', 'ThreeEstimates_score_perc',\n",
      "       'ThreeEstimates_score', 'optimal_perc_score', 'optimal_score',\n",
      "       'n_claims'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "for df in df_final.values():\n",
    "    print(df.columns)\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
